{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Input data files are available in the read-only \"../input/\" directory\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../input/sba-loans-case-data-set/SBAcase.11.13.17.csv\")\n",
    "\n",
    "df = df.select_dtypes(exclude = 'object')\n",
    "#removes 'ChgOffDate' and 'BalanceGross' features\n",
    "df = df.loc[:, df.columns != 'ChgOffDate']\n",
    "df = df.loc[:, df.columns != 'BalanceGross']\n",
    "df = df.loc[:, df.columns != 'Zip']\n",
    "#remove remaining NA values\n",
    "df = df.dropna()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:,0:df.shape[1]-1], df.iloc[:,df.shape[1]-1], test_size = .2)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "print('Loan Default percentage in training set: %.1f%%' % (100*np.mean(y_train)))\n",
    "print('Loan Default percentage in test set: %.1f%%' % (100*np.mean(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 1342 is max size of training set, with 20% removed for cross validation\n",
    "bigtrain_sizes = list(range(1,int(len(x_train)*.8)))\n",
    "train_sizes = [1]\n",
    "for i in bigtrain_sizes:\n",
    "    if i % 100 == 0:\n",
    "        train_sizes.append(int(i))\n",
    "train_sizes.append(1342)\n",
    "\n",
    "\n",
    "clf = MLPClassifier(batch_size=10, verbose=False, early_stopping=True)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "train_sizes, train_scores, validation_scores = learning_curve(estimator= clf, X = x_train_scaled, y = y_train, train_sizes = train_sizes, cv = cv, scoring = 'f1', verbose = 0)\n",
    "train_scores_mean = train_scores.mean(axis =1)\n",
    "validation_scores_mean = validation_scores.mean(axis = 1)\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = \"Training Error\")\n",
    "plt.plot(train_sizes, validation_scores_mean, label = \"Validation Error\")\n",
    "plt.ylabel('f1', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title(\"Learning curves\", fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "x_train_, x_val, y_train_, y_val = train_test_split(x_train_scaled, y_train, test_size = .2)\n",
    "\n",
    "# hidden_layer_sizes params\n",
    "bighidden_sizes = list(range(1,501))\n",
    "hidden_param_range = [1]\n",
    "for i in bighidden_sizes:\n",
    "    if i % 20 == 0:\n",
    "        hidden_param_range.append(int(i))\n",
    "param_range_len = np.arange(len(hidden_param_range))\n",
    "\n",
    "# solver params\n",
    "solver_param_range = ('lbfgs', 'sgd', 'adam')\n",
    "\n",
    "# activation params\n",
    "act_param_range = ('identity', 'logistic', 'tanh', 'relu')\n",
    "\n",
    "# learning_curve params\n",
    "learn_param_range = ('constant', 'invscaling', 'adaptive')\n",
    "\n",
    "param_range = act_param_range\n",
    "\n",
    "clf = MLPClassifier(batch_size=10, hidden_layer_sizes = [50], solver = 'lbfgs', activation = 'identity', verbose=False, early_stopping=True)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "train_scores, validation_scores = validation_curve(estimator= clf, X = x_train_, y = y_train_, param_name = 'activation', param_range = param_range, cv = cv, scoring = 'accuracy', verbose = 0)\n",
    "train_scores_mean = train_scores.mean(axis =1)\n",
    "validation_scores_mean = validation_scores.mean(axis = 1)\n",
    "# print(pd.Series(train_scores_mean, index = train_sizes))\n",
    "# print(pd.Series(validation_scores_mean, index = train_sizes))\n",
    "print(validation_scores_mean)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(param_range, train_scores_mean, label = \"Training Error\")\n",
    "plt.plot(param_range, validation_scores_mean, label = \"Validation Error\")\n",
    "plt.ylabel('F1 Score', fontsize = 14)\n",
    "plt.xlabel('Activation function', fontsize = 14)\n",
    "plt.title(\"Learning curves\", fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(.7,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, log_loss, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "x_train_, x_val, y_train_, y_val = train_test_split(x_train_scaled, y_train, test_size = .2)\n",
    "\n",
    "#dummy model accuracy (baseline)\n",
    "dummy_model = DummyClassifier(strategy = \"most_frequent\")\n",
    "dummy_model.fit(x_train_scaled,y_train)\n",
    "dummy_acc = dummy_model.score(x_test_scaled,y_test)*100\n",
    "dumb_pred = dummy_model.predict(x_test_scaled)\n",
    "print('Dummy model accuracy: %.3f%%' % dummy_acc)\n",
    "\n",
    "def getModel():\n",
    "    model = MLPClassifier(batch_size=10, solver = 'lbfgs', learning_rate = 'invscaling', activation = 'identity', hidden_layer_sizes = 1, verbose=False, early_stopping=True)\n",
    "    model.fit(x_train_,y_train_)\n",
    "    return model\n",
    "    \n",
    "model = getModel()\n",
    "\n",
    "# 10 fold cross validation score\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "results = cross_val_score(model, x_train_, y_train_, cv=cv)\n",
    "print('Training set k-fold cross validation mean accuracy: %.3f%%' % (100*np.mean(results)))\n",
    "\n",
    "#basic predict - validation set\n",
    "y_pred1 = model.predict(x_val)\n",
    "pred_accuracy_percentage1 = 100 * accuracy_score(y_val,y_pred1)\n",
    "print('Validation set accuracy: %.3f%%' % pred_accuracy_percentage1)\n",
    "\n",
    "#basic predict - test set\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "pred_accuracy_percentage = 100 * accuracy_score(y_test,y_pred)\n",
    "print('Test set accuracy: %.3f%%' % pred_accuracy_percentage)\n",
    "\n",
    "#ROC AUC metric\n",
    "# pred_auc = model.predict(x_test)\n",
    "pred_auc = model.predict_proba(x_test_scaled)[:,1]\n",
    "acc = roc_auc_score(y_test, pred_auc)\n",
    "print('AUC: %.3f%%' % (100*acc))\n",
    "\n",
    "#log loss metric\n",
    "lloss = log_loss(y_test,pred_auc, normalize = True)\n",
    "print('Log loss: %.3f' % lloss)\n",
    "\n",
    "#f1_score\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print('F1 Score: %.3f' % f1)\n",
    "\n",
    "#Confusion Matrix\n",
    "plt.figure()\n",
    "ax = plt.axes()\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', ax = ax)\n",
    "ax.set_title('Confusion Matrix of Loan Default Classifier')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
